FOLDER: ./experiments_t2m
TEST_FOLDER: ./experiments_t2m_test
NAME: motionlcm_humanml
SEED_VALUE: 1234
TRAIN:
  BATCH_SIZE: 128
  SPLIT: train
  NUM_WORKERS: 8
  PERSISTENT_WORKERS: true
  PRETRAINED: experiments_t2m/mld_humanml/mld_humanml.ckpt
  validation_steps: -1
  validation_epochs: 50
  checkpointing_steps: -1
  checkpointing_epochs: 50
  max_train_steps: -1
  max_train_epochs: 1000
  learning_rate: 0.0002
  lr_scheduler: cosine
  lr_warmup_steps: 1000
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_weight_decay: 0.0
  adam_epsilon: 1.0e-08
  max_grad_norm: 1.0
  w_min: 5.0
  w_max: 15.0
  num_ddim_timesteps: 10
  loss_type: huber
  huber_c: 0.5
  unet_time_cond_proj_dim: 256
  ema_decay: 0.95
VAL:
  BATCH_SIZE: 32
  SPLIT: test
  NUM_WORKERS: 12
  PERSISTENT_WORKERS: true
TEST:
  BATCH_SIZE: 32
  SPLIT: test
  NUM_WORKERS: 12
  PERSISTENT_WORKERS: true
  CHECKPOINTS: experiments_t2m/motionlcm_humanml/motionlcm_humanml.ckpt
  REPLICATION_TIMES: 20
  MM_NUM_SAMPLES: 100
  MM_NUM_REPEATS: 30
  MM_NUM_TIMES: 10
  DIVERSITY_TIMES: 300
  DO_MM_TEST: true
DATASET:
  NAME: humanml3d
  SMPL_PATH: ./deps/smpl
  WORD_VERTILIZER_PATH: ./deps/glove/
  HUMANML3D:
    FRAME_RATE: 20.0
    UNIT_LEN: 4
    ROOT: ./datasets/humanml3d
    CONTROL_ARGS:
      CONTROL: false
      TEMPORAL: false
      TRAIN_JOINTS:
      - 0
      TEST_JOINTS:
      - 0
      TRAIN_DENSITY: random
      TEST_DENSITY: 100
      MEAN_STD_PATH: ./datasets/humanml_spatial_norm
  SAMPLER:
    MAX_LEN: 200
    MIN_LEN: 40
    MAX_TEXT_LEN: 20
  PADDING_TO_MAX: false
  WINDOW_SIZE: null
METRIC:
  DIST_SYNC_ON_STEP: true
  TYPE:
  - TM2TMetrics
model:
  target:
  - motion_vae
  - text_encoder
  - denoiser
  - scheduler_lcm
  - noise_optimizer
  latent_dim:
  - 16
  - 32
  guidance_scale: dynamic
  t2m_textencoder:
    dim_word: 300
    dim_pos_ohot: 15
    dim_text_hidden: 512
    dim_coemb_hidden: 512
  t2m_motionencoder:
    dim_move_hidden: 512
    dim_move_latent: 512
    dim_motion_hidden: 1024
    dim_motion_latent: 512
  bert_path: ./deps/distilbert-base-uncased
  clip_path: ./deps/clip-vit-large-patch14
  t5_path: ./deps/sentence-t5-large
  t2m_path: ./deps/t2m/
  motion_vae:
    target: mld.models.architectures.mld_vae.MldVae
    params:
      nfeats: ${DATASET.NFEATS}
      latent_dim: ${model.latent_dim}
      hidden_dim: 256
      force_pre_post_proj: true
      ff_size: 1024
      num_layers: 9
      num_heads: 4
      dropout: 0.1
      arch: encoder_decoder
      normalize_before: false
      norm_eps: 1.0e-05
      activation: gelu
      norm_post: true
      activation_post: null
      position_embedding: learned
  text_encoder:
    target: mld.models.architectures.mld_clip.MldTextEncoder
    params:
      last_hidden_state: false
      modelpath: ${model.t5_path}
  denoiser:
    target: mld.models.architectures.mld_denoiser.MldDenoiser
    params:
      latent_dim: ${model.latent_dim}
      hidden_dim: 256
      text_dim: 768
      time_dim: 768
      ff_size: 1024
      num_layers: 9
      num_heads: 4
      dropout: 0.1
      normalize_before: false
      norm_eps: 1.0e-05
      activation: gelu
      norm_post: true
      activation_post: null
      flip_sin_to_cos: true
      freq_shift: 0
      time_act_fn: silu
      time_post_act_fn: null
      position_embedding: learned
      arch: trans_enc
      add_mem_pos: true
      force_pre_post_proj: true
      text_act_fn: null
      zero_init_cond: true
      controlnet_embed_dim: 256
      controlnet_act_fn: silu
  scheduler:
    target: mld.models.schedulers.scheduling_lcm.LCMScheduler
    num_inference_steps: 1
    cfg_step_map:
      1: 8.0
      2: 12.5
      4: 13.5
    params:
      num_train_timesteps: 1000
      beta_start: 0.00085
      beta_end: 0.012
      beta_schedule: scaled_linear
      clip_sample: false
      set_alpha_to_one: false
      original_inference_steps: 10
      timesteps_step_map:
        1:
        - 799
        2:
        - 699
        - 299
        4:
        - 699
        - 399
        - 299
        - 299
  noise_optimizer:
    target: mld.models.architectures.dno.DNO
    params:
      optimize: false
      max_train_steps: 400
      learning_rate: 0.1
      lr_scheduler: cosine
      lr_warmup_steps: 50
      clip_grad: true
      loss_hint_type: l2
      loss_diff_penalty: 0.0
      loss_correlate_penalty: 100
      visualize_samples: 0
      visualize_ske_steps: []
      output_dir: ${output_dir}
target:
- motion_vae
- text_encoder
- denoiser
- scheduler_lcm
- noise_optimizer
latent_dim:
- 16
- 32
guidance_scale: dynamic
t2m_textencoder:
  dim_word: 300
  dim_pos_ohot: 15
  dim_text_hidden: 512
  dim_coemb_hidden: 512
t2m_motionencoder:
  dim_move_hidden: 512
  dim_move_latent: 512
  dim_motion_hidden: 1024
  dim_motion_latent: 512
bert_path: ./deps/distilbert-base-uncased
clip_path: ./deps/clip-vit-large-patch14
t5_path: ./deps/sentence-t5-large
t2m_path: ./deps/t2m/
motion_vae:
  target: mld.models.architectures.mld_vae.MldVae
  params:
    nfeats: ${DATASET.NFEATS}
    latent_dim: ${model.latent_dim}
    hidden_dim: 256
    force_pre_post_proj: true
    ff_size: 1024
    num_layers: 9
    num_heads: 4
    dropout: 0.1
    arch: encoder_decoder
    normalize_before: false
    norm_eps: 1.0e-05
    activation: gelu
    norm_post: true
    activation_post: null
    position_embedding: learned
text_encoder:
  target: mld.models.architectures.mld_clip.MldTextEncoder
  params:
    last_hidden_state: false
    modelpath: ${model.t5_path}
denoiser:
  target: mld.models.architectures.mld_denoiser.MldDenoiser
  params:
    latent_dim: ${model.latent_dim}
    hidden_dim: 256
    text_dim: 768
    time_dim: 768
    ff_size: 1024
    num_layers: 9
    num_heads: 4
    dropout: 0.1
    normalize_before: false
    norm_eps: 1.0e-05
    activation: gelu
    norm_post: true
    activation_post: null
    flip_sin_to_cos: true
    freq_shift: 0
    time_act_fn: silu
    time_post_act_fn: null
    position_embedding: learned
    arch: trans_enc
    add_mem_pos: true
    force_pre_post_proj: true
    text_act_fn: null
    zero_init_cond: true
    controlnet_embed_dim: 256
    controlnet_act_fn: silu
scheduler:
  target: mld.models.schedulers.scheduling_lcm.LCMScheduler
  num_inference_steps: 1
  cfg_step_map:
    1: 8.0
    2: 12.5
    4: 13.5
  params:
    num_train_timesteps: 1000
    beta_start: 0.00085
    beta_end: 0.012
    beta_schedule: scaled_linear
    clip_sample: false
    set_alpha_to_one: false
    original_inference_steps: 10
    timesteps_step_map:
      1:
      - 799
      2:
      - 699
      - 299
      4:
      - 699
      - 399
      - 299
      - 299
noise_optimizer:
  target: mld.models.architectures.dno.DNO
  params:
    optimize: false
    max_train_steps: 400
    learning_rate: 0.1
    lr_scheduler: cosine
    lr_warmup_steps: 50
    clip_grad: true
    loss_hint_type: l2
    loss_diff_penalty: 0.0
    loss_correlate_penalty: 100
    visualize_samples: 0
    visualize_ske_steps: []
    output_dir: ${output_dir}
example: assets/eeg_example.txt
example_hint: null
no_plot: false
replication: 1
vis: tb
optimize: false
output_dir: ./experiments_t2m_test/motionlcm_humanml/demo_2026-01-12T13-44-07
